# Query: gpt-4o
# ContextLines: 1

20개 결과 - 6 파일

answer_agent.py:
  161                  temperature=0.0,
  162:                 model="gpt-5-mini-2025-08-07",
  163                  max_output_tokens=500,

  278              response = openai.chat.completions.create(
  279:                 model="gpt-5-mini-2025-08-07",
  280                  messages=[{"role": "user", "content": simple_prompt}],

_code\enhanced_llm_generator.py:
  311                  messages, schema, temperature=0.7,
  312:                 model="gpt-5-mini-2025-08-07", max_output_tokens=2000,
  313                  use_responses_api=False

  405                  messages, schema, temperature=0.1,
  406:                 model="gpt-5-mini-2025-08-07", max_output_tokens=1500,
  407                  use_responses_api=False

_code\llm_adapter.py:
  161      strict_schema = _ensure_schema_strict(schema)
  162:     chosen_model = model or os.environ.get("OPENAI_MODEL_PARAPHRASE", "gpt-5-mini-2025-08-07")
  163      cfg = _ClientConfig.from_env()

  325  
  326:     model = os.environ.get("OPENAI_MODEL_PARAPHRASE", "gpt-5-mini-2025-08-07")
  327      data = _call_llm_json(messages, schema, temperature=0.6, model=model, max_output_tokens=800, use_responses_api=False)

  421  
  422:     model = os.environ.get("OPENAI_MODEL_SYNTH", "gpt-5-mini-2025-08-07")
  423      data = _call_llm_json(

  497              messages, schema, temperature=0.7,
  498:             model=os.environ.get("OPENAI_MODEL_QUESTION", "gpt-5-mini-2025-08-07"),
  499              max_output_tokens=1200, use_responses_api=False

  835          try:
  836:             data = _call_llm_json(messages, schema, temperature=0.0, model=os.environ.get("OPENAI_MODEL_INTENT","gpt-5-mini-2025-08-07"), max_output_tokens=500, use_responses_api=False)
  837              if isinstance(data, dict):

  896      try:
  897:         data = _call_llm_json(messages, schema, temperature=0.0, model=os.environ.get("OPENAI_MODEL_HYPO_REVIEW","gpt-5-mini-2025-08-07"), max_output_tokens=1200, use_responses_api=False)
  898          if isinstance(data, list):

generators\enhanced_llm_generator.py:
  519                  messages, schema, temperature=0.7,
  520:                 model="gpt-5-mini-2025-08-07", max_output_tokens=2000,
  521                  use_responses_api=False

  618                  messages, schema, temperature=0.1,
  619:                 model="gpt-5-mini-2025-08-07", max_output_tokens=1500,
  620                  use_responses_api=False

utils\llm_adapter.py:
  161      strict_schema = _ensure_schema_strict(schema)
  162:     chosen_model = model or os.environ.get("OPENAI_MODEL_PARAPHRASE", "gpt-5-mini-2025-08-07")
  163      cfg = _ClientConfig.from_env()

  343  
  344:     model = os.environ.get("OPENAI_MODEL_PARAPHRASE", "gpt-5-mini-2025-08-07")
  345      data = _call_llm_json(messages, schema, temperature=0.6, model=model, max_output_tokens=800, use_responses_api=False)

  439  
  440:     model = os.environ.get("OPENAI_MODEL_SYNTH", "gpt-5-mini-2025-08-07")
  441      data = _call_llm_json(

  515              messages, schema, temperature=0.7,
  516:             model=os.environ.get("OPENAI_MODEL_QUESTION", "gpt-5-mini-2025-08-07"),
  517              max_output_tokens=1200, use_responses_api=False

  853          try:
  854:             data = _call_llm_json(messages, schema, temperature=0.0, model=os.environ.get("OPENAI_MODEL_INTENT","gpt-5-mini-2025-08-07"), max_output_tokens=500, use_responses_api=False)
  855              if isinstance(data, dict):

  914      try:
  915:         data = _call_llm_json(messages, schema, temperature=0.0, model=os.environ.get("OPENAI_MODEL_HYPO_REVIEW","gpt-5-mini-2025-08-07"), max_output_tokens=1200, use_responses_api=False)
  916          if isinstance(data, list):

utils\simple_generator.py:
  364                  messages, schema, temperature=0.7,
  365:                 model="gpt-5-mini-2025-08-07", max_output_tokens=1200,
  366                  use_responses_api=False

  544                  messages, schema, temperature=0.1,
  545:                 model="gpt-5-mini-2025-08-07", max_output_tokens=1500,
  546                  use_responses_api=False
